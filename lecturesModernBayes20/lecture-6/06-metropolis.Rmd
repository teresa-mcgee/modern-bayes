
---
title: "Module 6: Introduction to Metropolis"
author: "Rebecca C. Steorts"
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
---


```{r setup, include=TRUE}
knitr::opts_chunk$set(cache=TRUE)
```

Agenda
===
- Motivation
- Markov chain Monte Carlo (MCMC)
- Hard Discs in a Box Example
- Metropolis Algorithm
- Example Applied to Normal-Normal
- Practice Excerise (Hoff 10.3)

Intro to Markov chain Monte Carlo (MCMC)
===
Goal: sample from $f(x)$, or approximate $E_f[h(X)].$
\vskip 1em

Recall that $f(x)$ is very complicated and hard to sample from.

\vskip 1em

How to deal with this?
\begin{enumerate}
\item What's a simple way?
\item What is another way? 
\item What happens in high dimensions?
\end{enumerate}

High dimensional spaces
===
\begin{itemize}
\item In low dimensions, rejection sampling works pretty well. 
\item But in high dimensions we run into problems.
\item Why? It's hard to capture high dimensional spaces! 
\end{itemize}

\begin{figure}
  \begin{center}
    \includegraphics[width=0.3\textwidth]{figures/eye}
    \caption{A high dimensional space (many images). }
    \end{center}
\end{figure}

We turn to Markov chain Monte Carlo (MCMC).


Intuition
===
\begin{figure}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{figures/markovChainPoint}
    \caption{Example of a Markov chain and red starting point}
    \end{center}
\end{figure}

Intuition
===
\begin{figure}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{figures/markovChainMove}
    \caption{Example of a Markov chain and moving from the starting point to a high probability region.}
    \end{center}
\end{figure}

What is Markov Chain Monte Carlo
===
\begin{itemize}
\item Markov Chain -- where we go next only depends on our last state (the Markov property).
\item Monte Carlo -- just simulating data. 
\end{itemize}

The Markov property
===

Suppose that we have just visited states $x_1,\ldots,x_{n-1}.$ The Markov property says the following:

$$P(X_n = x_n \mid X_1 = x_1,\ldots,X_{n-1}=x_{n-1}) = P(X_n = x_n \mid X_{n-1}=x_{n-1}).$$

Why MCMC?
===
\begin{itemize}
\item[(a)] the region of high probability tends to be ``connected''
\item That is, we can get from one point to another without going through a low-probability region, and
\item[(b)] we tend to be interested in the expectations of functions that are relatively smooth and have lots of ``symmetries''
\item  That is, one only needs to evaluate them at a small number of representative points in order to get the general picture.
\end{itemize}

Advantages/Disadvantages of MCMC:
===
Advantages:
\begin{itemize}
\item applicable even when we can't directly draw samples
\item works for complicated distributions in high-dimensional spaces, even when we don't know where the regions of high probability are
\item relatively easy to implement
\item fairly reliable
\end{itemize}

Disadvantages:
\begin{itemize}
\item slower than simple Monte Carlo or importance sampling (i.e., requires more samples for the same level of accuracy)
\item can be very difficult to assess accuracy and evaluate convergence, even empirically
\end{itemize}

Hard Discs in a Box Example
===
\begin{figure}
  \begin{center}
    \includegraphics[width=0.8\textwidth]{figures/phaseDiagram}
    \caption{Example of a phase diagram in chemistry.}
    \end{center}
\end{figure}

Many materials have phase diagrams that look like the picture above. 

Hard Discs in a Box Example
===
To understand this phenoma, a theoretical model was proposed:
\href{http://www.aliquote.org/pub/metropolis-et-al-1953.pdf}{Metropolis, Rosenbluth, Rosenbluth, and Teller, 1953}

\vskip 1em


\begin{figure}
  \begin{center}
    \includegraphics[width=0.4\textwidth]{figures/hardDiscs}
    \caption{Example of $N$ molecules (hard discs) bouncing around in a box.}
    \end{center}
\end{figure}
\vskip 1em

Called hard discs because the molecules cannot overlap. 

Hard Discs in a Box Example
===
Have an $X = (u,v)$ coordinate for each molecule.\footnote{For the rest of the lecture, $X$ will denote the two coordinate vectors of the molecule.}
\vskip 1em
The total dimension of the space is $\mathbb{R}^{2N}.$

\begin{figure}
  \begin{center}
    \includegraphics[width=0.4\textwidth]{figures/hardDiscs}
    \caption{Example of $N$ molecules (hard discs) bouncing around in a box.}
    \end{center}
\end{figure}

Hard Discs in a Box Example
===
$X \sim f(x)$ (Boltzman distribution). 

\vskip 1em
Goal: compute $E_f[h(x)].$

\vskip 1em
Since $X$ is high dimensional, they proposed ``clever moves" of the molecules. 

Hard Discs in a Box Example
===
Metropolis algorithm: For iterations $i=1,\ldots,n$, do:
\begin{enumerate}
\item Consider a molecule and a box around the molecule.
\item Uniformly draw a point in the box.
\item According to a ``rule", you accept or reject the point. 
\item If it's accepted, you move the molecule. 
\end{enumerate}

Uniformly = pick a point at random with equal probability to all other points in the box

Example of one iteration of algorithm
===
Consider a molecule and a box around the molecule.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.4\textwidth]{figures/hardDiscsUniform}
    \caption{This illustrates step 1 of the algorithm.}
    \end{center}
\end{figure}

Example of one iteration of algorithm
===
Uniformly draw a point in the box.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.4\textwidth]{figures/hardDiscsPoint}
    \caption{This illustrates step 2 of the algorithm.}
    \end{center}
\end{figure}

Example of one iteration of algorithm
===
According to a ``rule", you accept or reject the point. 

\vskip 1em

Here, it was accepted, so we move the molecule. 

\begin{figure}
  \begin{center}
    \includegraphics[width=0.4\textwidth]{figures/hardDiscsAccept}
    \caption{This illustrates step 3 and 4 of the algorithm.}
    \end{center}
\end{figure}

Example of one iteration of algorithm
===
Here, we show one entire iteration of the algorithm.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.4\textwidth]{figures/hardDiscsMetropolis}
    \caption{This illustrates one iteration of the algorithm.}
    \end{center}
\end{figure}

After running many iterations $n$ (not $N$), we have an approximation for $E_f(h(X)),$ which is
$\frac{1}{n} \sum_i h(X_i).$

We will talk about the details later of why this is a ``good approximation."

Some food for thought
===
We just covered the Metropolis algorithm (1953 paper). 
\begin{itemize}
\item We did not cover the exact procedure for accepting or rejecting (to come). 
\item Are the $X_i$'s independent? 
\item Our approximation holds by \href{https://www.youtube.com/watch?v=ZjrJpkD3o1w}{The Ergodic Theorem} \\
for those that want to learn more about it.
\item The ergodic theorem says: ``if we start at a point $x_o$ and we keeping moving around in our high dimensional space, then we are guaranteed to eventually reach all points in that space with probability 1."
\end{itemize}

Metropolis Algorithm
===
Setup: Assume pmf $\pi$ on $\mathcal{X}$ (countable).

\vskip 1em

Have $f: \mathcal{X} \rightarrow \mathbb{R}.$

\vskip 1em

Goal: 
\begin{enumerate}
\item[a)] sample/approximate from $\pi$
\item[b)] approximate $E_\pi[f(x)], X\sim \pi.$
\end{enumerate}

The assumption is that $\pi$ and or $f(X)$ are complicated!

Why things work!
===
Big idea and why it works: we apply the ergodic theorem.
\vskip 1em

``If we take samples $X = (X_0,X_1,\ldots,)$ then by the ergodic theorem, they will eventually reach $\pi$, which is known as the stationary distribution (the true pmf)."


Metropolis Algorithm
===
The approach is to apply the ergodic theorem. 

\begin{enumerate}
\item If we run the Markov chain long enough, then the last state is approximately from $\pi.$
\item Under some regularity conditions, 
$$\frac{1}{n}\sum_{i=1}^n f(X_i) \stackrel{a.s}{\longrightarrow} E_\pi[f(x)].$$
\end{enumerate}

Terminology
===
\begin{enumerate}

\item Proposal matrix = stochastic matrix. 

\vskip 1em

Let $$Q = (Q_{ab}: a,b \in \mathcal{X}).$$

\vskip 1em

Note: I will use $Q_{ab} = Q(a,b)$ at times. 

\item Note: $$\pi(x) = \tilde{\pi}(x)/z, z>0.$$
What is known and unknown above? (Think back to rejection sampling)
\end{enumerate}


Metropolis Algorithm
===
Assume a symmetric proposal matrix $Q.$ So, $Q_{ab} = Q_{ba}.$

Initialize a starting point $x_o \in X.$

\vspace*{1em}

Goal: run $n$ total iterations of the algorithm to produce $(x_o, x_1, \ldots, x_n)$ samples. 

\vspace*{1em}

This is useful as $$\sum_{i=1}^n f(x_i) \approx E_\pi[f(x)].$$

Metropolis Algorithm
===

For \textcolor{red}{$i \in 1,2,\ldots,n$:}
\begin{itemize}
\item Sample a new data point $x^{*}$ from $Q(x_i, x^{*})$ if $x$ is discrete, otherwise, $p(x^{*} \mid x_i).$
\item Sample $\textcolor{red}{r}$ from Uniform$(0,1).$
\item If $$r < \frac{\tilde{\pi}(x^{*})}{\tilde{\pi}(x_i)},$$ accept and $x_{i+1} = x^{*}.$ \textcolor{red}{accept the new data point}
\item Otherwise, reject and $x_{i+1} = x_i.$ \textcolor{red}{keep the current data point}
\end{itemize}
Output: $x_o,x_1,\ldots,x_n$

Comment: $r$ is the rule for uniformly drawing a point in the box (slide 18).

Metropolis Algorithm (Intuition) 
===

How do we decide to accept or reject $x^{*}?$ 

\vspace*{1em}

In short, what is the intuition regarding how one would construct the Metropolis algorithm? \footnote{This adaptation is from Hoff, page 174, with credit due completely to Hoff.}

Metropolis Algorithm (Intuition) 
===

Suppose that we are considering to add a new data point $x^{*}$ that is close to our current state $x_i.$

\vspace*{1em}

1. If $\tilde{\pi}(x^{*}) > \tilde{\pi}(x_i),$ then we would want to add a data point that is ``close to" an existing data point in our set $x_i.$ 

\vspace*{1em}

2. However, if $\tilde{\pi}(x^{*}) < \tilde{\pi}(x_i),$ this case is not as clear cut. In short, we will want to accept/reject $x^{*}$ in a logical way. (We will walk through this in detail below).

Metropolis Algorithm (Intuition) 
===

Define $$r = \frac{\tilde{\pi}(x^{*})}{\tilde{\pi}(x_i)}.$$

1. Suppose r > 1. 

\textbf{Idea}: Since $x_i$ is in our data set already, we should include $x^{*}$ since it has higher probability than $x_i.$

\textbf{Procedure}: Accept $x^{*},$ and update $x_{i+1} = x^{*}.$

\textbf{Summary}: We always accept $x^{*}$ when $r>1.$ (This is the trivial case). 

Metropolis Algorithm (Intuition) 
===

2. Suppose r < 1.

\textbf{Idea}: The relative frequency values in our set equal to $x^{*}$ compared to those equal to $x_{i}$ will be $r.$ This implies that for every instance of $x_{i}$ we should only have a fraction of an instance of a $x^{*}$ value. 

\textbf{Procedure}: Set $x_{i+1}$ to $x^{*}$ or $x_{i}$ with probability $r$ and $1-r,$ respectively. 

\textbf{Summary}: We will accept/reject the new data point $x_{i+1}$ with probability $r$ and $1-r.$

Metropolis Algorithm (Intuition) 
===

Reminder: We always accept in case 1, so we only need to consider case 2. 

$$
x_{i+1} = \begin{cases}
  x^{*}  &  \text{with probability} \quad \min(r,1)\\
  x_{i} &  \text{with probability} \quad 1 - \min(r,1)
\end{cases}
$$
\vspace*{1em}

We accomplish the sampling by drawing $u \sim \text{Uniform}(0,1).$ Then $x_{i+1} = x^{*}$ if $u < r.$ Otherwise, 
$x_{i+1} = x_{i}.$

Metropolis Algorithm (Correctness) 
===

The Metropolis Algorithm is ``correct"  as one can show that it satisfies the conditions of the Ergodic Theorem. 
(Details are sketched along with definitions at the end of the slides for those that might be interested.)


Metropolis within a Bayesian setting
===
Goal: We want to sample from $$p(\theta \mid y)= \frac{f(y \mid \theta) \pi(\theta)}{m(y)}.$$

\vskip 1em

Typically, we don't know $m(y).$

\vskip 1em

The notation is a bit more complicated, but the  set up is the same.

\vskip 1em 

We'll approach it a bit differently, but the idea is exactly the same. 

Building a Metropolis sampler
===
We know $\pi(\theta)$ and $f(y\mid \theta)$, so we can can draw samples from these. 

\vskip 1 em

Our notation here will be that we assume parameter values $\theta_1,\theta_2,\ldots,\theta_s$ which are drawn from $\pi(\theta).$

\vskip 1 em

We assume a new parameter value comes in that is $\theta^*.$

Building a Metropolis sampler
===
Similar to before we assume a symmetric proposal distribution, which we call $J(\theta^* \mid \theta^{(s)}).$

\vskip 1 em

\begin{itemize}
%\item It proceeds by sampling a proposal value $\theta^*$ nearby the current value 
%$\theta^{(s)}$ using a \emph{symmetric proposal distribution} $J(\theta^* \mid \theta^{(s)}).$
\item What does symmetry mean here? 
$J(\theta_a \mid \theta_b) = J(\theta_b \mid \theta_a).$ 
\item That is, the probability of proposing $\theta^* = \theta_a$ given that $\theta^{(s)} = \theta_b$ is equal to the probability of proposing 
$\theta^* = \theta_b$ given that $\theta^{(s)} = \theta_a.$
\item Symmetric proposals include:\\
$$J(\theta^*\mid \theta^{(s)}) = \text{Uniform}(\theta^{(s)} - \delta, \theta^{(s)} + \delta)$$
and
$$J(\theta^*\mid \theta^{(s)}) = \text{Normal}(\theta^{(s)}, \delta^2).$$
\end{itemize} 

Metropolis algorithm
===
The Metropolis algorithm proceeds as follows:
\begin{enumerate}
\item Sample $\theta^* \sim J(\theta \mid \theta^{(s)}).$
\item Compute the acceptance rule (r):
$$r = \dfrac{p(\theta^*|y)}{p(\theta^{(s)}|y)} = \dfrac{p(y \mid \theta^*)p(\theta^*)}
{p(y \mid \theta^{(s)})p(\theta^{(s)})}.
$$
\item Let $$\theta^{(s+1)} =\begin{cases}
\theta^* &\mbox{with prob min(r,1)}  \\
\theta^{(s)} & \mbox{otherwise. } \end{cases} 
$$
\end{enumerate}
Remark: Step 3 can be accomplished by sampling $u \sim \text{Uniform}(0,1)$ and setting 
$\theta^{(s+1)} = \theta^*$ if $u<r$ and setting $\theta^{(s+1)} = \theta^{(s)}$  otherwise.
\vskip 1em
Exercise: Convince yourselves that step 3 is the same as the remark!

A Toy Example of Metropolis
===
Let's test out the Metropolis algorithm for the conjugate Normal-Normal model with a known variance situation. 

 
\begin{align*}
X_1,\ldots,X_n\mid \theta &\stackrel{iid}{\sim} \text{Normal}(\theta, \sigma^2)\\
\theta &\sim \text{Normal}(\mu, \tau^2).\\
\end{align*}
Recall that the posterior of $\theta \mid X_1,\ldots,X_n$ is Normal$(\mu_n, \tau_n^2),$
where
$$\mu_n = \bar{x}\frac{n/\sigma^2}{n/\sigma^2+1/\tau^2}
+
 \mu \frac{1/\tau^2}{n/\sigma^2+1/\tau^2}$$
 and 
 $$ \tau_n^2 = \frac{1}{n/\sigma^2+1/\tau^2}.$$
 
Toy Example
===
In this example: $\sigma^2 =1,$ $\tau^2 = 10,$ $\mu =5,$ $n = 5,$ and 
$$x = (9.37, 10.18,9.16,11.60,10.33).$$ For these data, $\mu_n = 10.03$ and 
$\tau^2_n = 0.20.$

\vskip 1em

Note: this is a toy example for illustration. 

Toy example
===
We need to compute the acceptance rule $r.$

\begin{align}
r &= \dfrac{p(\theta^{*}|x)}{p(\theta^{(s)}|x)} \\
&= 
\dfrac{
p(x|\theta^{*})p(\theta^{*})
}
{
p(x|\theta^{(s)})
p(\theta^{(s)})
}\\
&=\left( \frac{\prod_i\text{dnorm}(x_i, \theta^*,\sigma)}
{\prod_i \text{dnorm}(x_i, \theta^{(s)},\sigma)}
\right)
\left( \frac{ \text{dnorm}(\theta^*, \mu, \tau)}
{ \text{dnorm}(\theta^{(s)}, \mu, \tau)}
\right)
\end{align}

Toy example
===
In many cases, computing the rule $r$ directly can be numerically unstable, however, this can be modified by taking $\log r.$ 



This results in 
\begin{align*}
&\log r =\sum_i\left[
\log  \text{dnorm}(x_i, \theta^*,\sigma) - 
\log  \text{dnorm}(x_i, \theta^{(s)},\sigma)
\right] \\
&\qquad+
\left[
\log  \text{dnorm}(\theta^*, \mu, \tau)\right] - 
\log  \text{dnorm}(\theta^{(s)}, \mu, \tau)
.
\end{align*}
Then a proposal is accepted if $\log u < \log r,$ where $u$ is sampled from the Uniform(0,1).

Toy example
===
We generate 500 iterations of the Metropolis algorithm starting at 
$\theta^{(0)} = 0$ and using a normal proposal distribution, where
$$\theta^{(s+1)} \sim \text{Normal}(\theta^{(s)},1).$$

We will then generate 10,000 iterations since 500 will not be sufficient. 

Figure~\ref{met_norm} shows a traceplot for this run as well as a histogram for the Metropolis algorithm compared with a draw from the true normal density. 

Traceplot
===

What is a traceplot? A traceplot is a convergence diagnostic.

It's a plot of the parameter of interest versus the number of MCMC iterations. 

What does it tell us?

1. It can tell us when we have not run our MCMC long enough.
2. It can tell us when we migth be in a situation of multimodality (we will see this in future lectures).
3. If the plots looks stable, then it tells us that we do not see anything to warrant issues with a lack of convergence of the chain. 

What does it not tell us? 

It cannot tell us that our MCMC has converged! 



Code
===
```{r}
# setting values 
set.seed(1)
s2<-1 
t2<-10 
mu<-5; 
n<-5

# defining the data
x<-c(9.37, 10.18, 9.16, 11.60, 10.33)
# mean of the normal posterior
mu.n<-( mean(x)*n/s2 + mu/t2 )/( n/s2+1/t2) 
# variance of the normal posterior 
t2.n<-1/(n/s2+1/t2)
```

Code (continued)
===
```{r}
##S = total num of simulations
theta<-0 ; delta<-1 ; S<-50 ; THETA<-NULL

for(s in 1:S){
## simulating our proposal
theta.star<-rnorm(1,theta,sqrt(delta))
##taking the log of the ratio r
log.r<-( sum(dnorm(x,theta.star,sqrt(s2),log=TRUE)) +
dnorm(theta.star,mu,sqrt(t2),log=TRUE) ) -
( sum(dnorm(x,theta,sqrt(s2),log=TRUE)) +
dnorm(theta,mu,sqrt(t2),log=TRUE) )
if(log(runif(1))<log.r) { 
  theta<-theta.star 
}

THETA<-c(THETA,theta) ##updating THETA

}
```

Code (continued)
===
```{r}
length(THETA)
head(THETA)
```

Code (continued)
===
```{r, echo=FALSE}
# two plots: trace of theta and 
# comparing the empirical distribution
# of simulated values to the true posterior
pdf("metropolis_normal-500.pdf",
    family="Times",height=3.5,width=7)
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
par(mfrow=c(1,2))
# creating a sequence
skeep<-seq(10,S,by=10)
# making a trace place
plot(skeep,THETA[skeep],type="l",
xlab="iteration",ylab=expression(theta))
# making a histogram
hist(THETA[-(1:50)],prob=TRUE,main="",
xlab=expression(theta),ylab="density")
th<-seq(min(THETA),max(THETA),length=100)
lines(th,dnorm(th,mu.n,sqrt(t2.n)) )
dev.off()
```

Code (continued)
===
```{r, echo=FALSE}
S <- 10000
for(s in 1:S)
{
## simulating our proposal
theta.star<-rnorm(1,theta,sqrt(delta))
##taking the log of the ratio r
log.r<-( sum(dnorm(x,theta.star,sqrt(s2),log=TRUE)) +
dnorm(theta.star,mu,sqrt(t2),log=TRUE) ) -
( sum(dnorm(x,theta,sqrt(s2),log=TRUE)) +
dnorm(theta,mu,sqrt(t2),log=TRUE) )
if(log(runif(1))<log.r) { theta<-theta.star }
##updating THETA
THETA<-c(THETA,theta)
}
# two plots: trace of theta and 
# comparing the empirical distribution
# of simulated values to the true posterior
pdf("metropolis_normal-10000.pdf",
    family="Times",height=3.5,width=7)
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
par(mfrow=c(1,2))
# creating a sequence
skeep<-seq(10,S,by=10)
# making a trace place
plot(skeep,THETA[skeep],type="l",
xlab="iteration",ylab=expression(theta))
# making a histogram
hist(THETA[-(1:50)],prob=TRUE,main="",
xlab=expression(theta),ylab="density")
th<-seq(min(THETA),max(THETA),length=100)
lines(th,dnorm(th,mu.n,sqrt(t2.n)) )
dev.off()
```

Traceplot and Histogram
===
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.5]{figures/metropolis_normal-500}
\caption{Left: trace plot of the Metropolis sampler. Right: Histogram versus true normal density for 500 iterations.}
\label{met_norm}
\end{center}
\end{figure}

Traceplot and Histogram
===
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.5]{figures/metropolis_normal-10000}
\caption{Left: trace plot of the Metropolis sampler. Right: Histogram versus true normal density for 10,000 iterations.}
\label{met_norm}
\end{center}
\end{figure}

Traceplot
===

Given that we have looked at $n=500$ iterations of the Metroplis sampler, does it seem that our approximation is a good one?

What about $n=10,000$ iterations? 

Questions you should be able to answer!
===
\begin{itemize}
\item What is the goal of Metropolis?
\item What is known and unknown? 
\item What are good proposals? 
\item What does the ergodic theorem say in words?
\item Are good proposals always easy to choose?
\item When would we use Metropolis over Importance sampling
and Rejection sampling? 
\item What is a simple diagnostic of a Markov chain?
\item Are we guaranteed convergence of the Markov chain?
\end{itemize}

Detailed Takeways
===

- Monte Carlo is not sufficient for high dimensions
- Intuitive of MCMC
- What is Monte Carlo
- What is a Markov Chain
- What is the memoryless property
- Why do we use MCMC
- Advantages versus Disdavantages of MCMC
- Metropolis et al. (1953) original paper and understanding this case study
- Ergodic Theorem 
- Metropolis Algorithm (from the 1953 paper)
- Metropolis Algorithm in a Bayesian setting
- Normal-Normal Example using Metropolis
- Traceplots: What they tells us and what they don't tells us
- How long should you run an MCMC chain
- How you would walk through a case study of metropolis but on the normal-uniform model (very similar to the in class study)


Information about Markov chains
===

If you're wondering what a Markov chain and a problem looks like in a discrete space, take a look at this video: \url{https://www.youtube.com/watch?v=i3AkTO9HLXo}

If you're wanting to read more about Markov chains, see \url{http://faculty.washington.edu/yenchic/18A_stat516/Lec3_DTMC_p1.pdf}

Ergodic Theorem
===

Suppose $(X_o, X_1, \ldots, X_n)$ is an irreducible (time-homoegenous) discrete Markov Chain with stationary distribution $\pi.$ 

Then $$\frac{1}{n} \sum_{i=1}^n f(X_i) \stackrel{a.s.}{\rightarrow} E_f[X]$$ as $n \rightarrow \infty$ where $X \sim \pi.$ Also, $f: \mathcal{X} \rightarrow R$ is a bounded function.

If the Markov Chain is aperiodic, then 
$$P(X_n = x \mid X_o = x_o) \rightarrow \pi(x)$$ as $n \rightarrow \infty$ for all $x, x_o \in \mathcal{X}.$

Notation
===

Let $(X_i) = (X_o, X_1, \ldots, X_n).$

Time-Homogeneous Markov Chain
===

A Markov Chain $(X_i)$ is time homogeneous if 
$$P(X_{i+1} = b \mid X_i = a) = T_{ab}$$ for all 
$i$, for all $a,b \in \mathcal{X},$ and for matrix T.

The matrix T is called the transition matrix (which is a stochastic matrix). 

Stationary Distribution
===

A pmf $\pi$ defined on $\mathcal{X}$ is a stationary distribution (with respect to the transition kernel) if 

$$\pi T = \pi.$$
Another way to write this is as follows:

$$\sum_{a \in \mathcal{X}} \pi_a T_{ab} = \pi_b$$ for all $b \in \mathcal{X}.$


Irreducible
===

A Markov Chain $(X_i)$ is irreducible if for all $a,b \in \mathcal{X}$ there exists $t \geq 0$ such that
$$P(X_t = b \mid X_o = a) > 0.$$


Aperiodic
===

An irreducible Markov chain $(X_i)$ is aperiodic if for all $a \in \mathcal{X}$

$$gcd\{
P(X_t = a \mid X_o \mid a) > 0
\} = 1.$$

$\text{greatest common divisor} = gcd$




Metropolis (Correctness)
===

We need to prove that the ergodic theorem holds. (I'll provide a sketch of the proof, which requires knowledge of measure theory/real analysis). 

The transition kernel can be written as follows:

\begin{align}
T(x,x_i) &= P(X_{i+1} = x \mid X_i = x_i)\\
&= p(x \mid x_i) \times p(\text{accept} \; x \mid x, x_i) \\
&= Q(x_i,x) min(1, \tilde{\pi}(x)/ \tilde{\pi}(x_i))
\end{align}

as long as $x_i \neq x.$

Proposition: Detailed Balance
===

$X_i$ has stationary distribution $\pi.$

Proof: Need to verify that $\pi_a T_{ab} = \pi_b T_{ba}.$

a. If $a=b$, the above trivially holds.
b. If $a\neq b,$ 
\begin{align}
\pi(a) T(a,b) &= \pi(a) Q(a,b) \min(1, \tilde{\pi}(x)/ \tilde{\pi}(x_i)) \\
&= Q(a,b) \min(\pi(a), \pi(b)) \quad \text{symmetric about a,b} \\
&= \pi(b) T(b,a)
\end{align}

Thus, detailed balance is satisfied. 

Result: Aperiodity and Irreduciblity
===

Assume $\pi(x) > 0$ for all $x \in \mathcal{X}.$


1. If Q is irreducible, then T is irreducible.

2. If Q is aperiodic, then T is aperiodic. 



Proposition
===
If $\pi(x) > 0$ for all $x \in \mathcal{X}$ and $Q$ is irreducible then
$$\frac{1}{n} \sum_{i=1}^n f(X_i) \stackrel{a.s.}{\rightarrow} E_f[X]$$ as $n \rightarrow \infty.$

Also, if Q is aperiodic, then

$$P(X_n = x \mid X_o = x_o) \rightarrow \pi(x)$$ as $n \rightarrow \infty$ for all $x \in \mathcal{X}.$