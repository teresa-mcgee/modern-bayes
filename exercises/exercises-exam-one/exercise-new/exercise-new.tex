\documentclass[12pt]{article} 
\input{../../custom}
\graphicspath{{figures/}}
\def\showcommentary{1}


\title{Practice Exercise 2023 }
\author{}
\date{}


\begin{document}
\maketitle



\subsection*{Exam I Practice Problem}
We write $X\sim\Poisson(\theta)$ if $X$ has the Poisson distribution with rate $\theta>0$, that is, its p.m.f.\ is
$$ p(x|\theta) = \Poisson(x|\theta)= e^{-\theta} \theta^x / x!$$
for $x\in\{0,1,2,\dotsc\}$ (and is $0$ otherwise). Suppose $X_1,\dotsc,X_n\iid\Poisson(\theta)$ given $\theta$, and your prior is
$$ p(\theta) =\Ga(\theta|a,b) =\frac{b^a}{\Gamma(a)} \theta^{a-1} e^{-b\theta}\I(\theta>0) .$$

\vspace*{1em}

\begin{enumerate}
\item \textbf{Derive} (step by step) $p( \theta \mid x_{1:n} )$ where $x_{1:n} = (X_1,\dotsc,X_n).$
\item Assume the squared error loss function. \textbf{Provide} the Bayes rule (under the model defined above). (You do not have to prove the result).
\item \textbf{Derive} (step by step) the posterior predictive distribution of $p(\tilde{x} \mid x_{1:n}),$ for a new observation $\tilde{x}.$ Identify the distribution of the posterior predictive and its parameters for full credit. 
\end{enumerate}

\textbf{Solutions}:
\begin{enumerate}
\item $$p(\theta \mid x_{1:n}) = \text{Gamma}(a_n = a + \sum_i x_i, b_n = b + n).$$
\item The Bayes' rule is the posterior mean, which is $\frac{a_n}{b_n}$ under the updated posterior distribution above. 
\item 
\begin{align}
p(\tilde{x} \mid x_{1:n}) &= \frac{\Gamma(\tilde{x} + a_n)}
{\Gamma(a_n)\tilde{x}!} \times b_n^{a_n} (\frac{1}{b_n+1})^{\tilde{x} + a_n}\\
&\implies
\tilde{x} \mid x_{1:n} \sim \text{NegativeBinomial}(a_n, \frac{b_n}{b_n + 1}).
\end{align}
\end{enumerate}




%\newpage
%\vfill
%\rotatebox{180}{
%\begin{minipage}[t][\textheight][t]{\textwidth}
%\subsection*{Partial Solution}
%\scriptsize
%Since the data is independent given $\theta$, the likelihood factors and we get
%\begin{align*}
%p(x_{1:n}|\theta) & = \prod_{i = 1}^n p(x_i|\theta) \\
%& = \prod_{i = 1}^n e^{-\theta} \theta^{x_i} / x_i! \\
%& \underset{\theta}{\propto} e^{-n\theta} \theta^{\sum x_i}.
%\end{align*}
%Thus, using Bayes' theorem,
%\begin{align*}
%p(\theta|x_{1:n}) &\propto p(x_{1:n}|\theta) p(\theta) \\
%& \propto e^{-n\theta} \theta^{\sum x_i} \theta^{a-1} e^{-b\theta}\I(\theta>0) \\
%& \propto e^{-(b+n)\theta} \theta^{a+\sum x_i-1}\I(\theta>0) \\
%& \propto \Ga\big(\theta\mid a+\textstyle\sum x_i,\,b+n\big).
%\end{align*}
%Therefore, since the posterior density must integrate to $1$, we have
%$$p(\theta|x_{1:n}) =\Ga\big(\theta\mid a+\textstyle\sum x_i,\,b+n\big).$$
%\end{minipage}}

\end{document}






